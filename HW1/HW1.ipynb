{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MVA_DM1_DHAOU_Amin.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO3XLhKoG_dZ",
        "colab_type": "text"
      },
      "source": [
        "# Linear classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpRefQyQHLXn",
        "colab_type": "text"
      },
      "source": [
        "The files $trainA$, $trainB$ and $trainC$ contain samples of data $(x_n,y_n)$ where $x_n∈$ $\\mathbb{R^2}$ and $y_n∈\\{0,1\\}$ (each line of each file contains the 2 components of $x_n$ then $y_n$). The goal of this exercise is to implement linear classification methods and to test them on the three data sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tReAoIRaIegm",
        "colab_type": "text"
      },
      "source": [
        "#### Util functions and libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3wA0Q_GNSTX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84XkHTMeIj2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigm(x):\n",
        "    \"\"\" \n",
        "    This function compute the sigmoid function. \n",
        "    \"\"\"\n",
        "    return 1/(1+np.exp(-x))\n",
        "def plot_classification(train,train,boundary_x,boundary_y,i, linear = True,\n",
        "                        model,Z=0,Zt=0):\n",
        "    \n",
        "    x_train = train[:,:-1]\n",
        "    y_train = train[:,2]\n",
        "    \n",
        "    x_test = test[:,:-1]\n",
        "    y_test = test[:,2]\n",
        "\n",
        "    X_1 = x_train[y_train == 1]\n",
        "    X_0 = x_train[y_train == 0]\n",
        "    Xt_1 = x_test[y_test == 1]\n",
        "    Xt_0 = x_test[y_test == 0]\n",
        "    fig, ax =plt.subplots(1,2)\n",
        "    #fig.suptitle(model +' on data ' + i )\n",
        "    ax[0].set_title(model + regression train set \" + i)\n",
        "    if linear:\n",
        "      ax[0].plot(boundary_x, boundary_y, c='g', label='Decision boundary')\n",
        "    else:\n",
        "      ax[0].contour(boundary_x0, boundary_x1,Z,[0] ,colors=\"g\")\n",
        "    ax[0].scatter(X_1[:, 0], X_1[:, 1], c='r', label='True label: 1')\n",
        "    ax[0].scatter(X_0[:, 0], X_0[:, 1], c='b', label='True label: 0')\n",
        "\n",
        "    ax[1].set_title(model+ \" test set \" + i)\n",
        "    if linear:\n",
        "      ax[0].plot(boundary_x, boundary_y, c='g', label='Decision boundary')\n",
        "    else:\n",
        "      ax[1].contour(boundary_xt0, boundary_xt1,Zt,[0], colors = \"g\")\n",
        "    ax[1].scatter(Xt_1[:, 0], Xt_1[:, 1], c='r', label='True label: 1')\n",
        "    ax[1].scatter(Xt_0[:, 0], Xt_0[:, 1], c='b', label='True label: 0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qAqjEfYjMSo",
        "colab_type": "text"
      },
      "source": [
        "In the following code cell, we will load the three Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bidbf2zVjQu4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Loading the datasets\n",
        "\n",
        "#Training sets\n",
        "trainA = np.array(pd.read_csv('trainA.csv', delimiter = \" \"))\n",
        "trainB = np.array(pd.read_csv('trainB.csv', delimiter = \" \"))\n",
        "trainC = np.array(pd.read_csv('trainC.csv', delimiter = \" \"))\n",
        "\n",
        "list_train = [trainA, trainB, trainC]\n",
        "\n",
        "#testing sets\n",
        "testA = np.array(pd.read_csv('testA.csv', delimiter = \" \"))\n",
        "testB = np.array(pd.read_csv('testB.csv', delimiter = \" \"))\n",
        "testC = np.array(pd.read_csv('testC.csv', delimiter = \" \"))\n",
        "\n",
        "list_test = [testA, testB, testC]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gf-8ATfHtiR",
        "colab_type": "text"
      },
      "source": [
        "### Generative Model (LDA)\n",
        "\n",
        "\n",
        "Given the class variable, the data are assumed to be Gaussian with different means for different classes but with the same covariance matrix.\n",
        "$$\\begin{aligned}\n",
        "  y∼Bernoulli(π) , x|y=i∼Normal(μ_i,Σ).\n",
        "\\end{aligned}$$\n",
        "\n",
        "\\\\\n",
        "\n",
        "####  Implementation of the MLE for this model and representation of the data and the classification line defined by the equation $p(y=1|x)=0.5$ : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmPyfT5fGmvS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_LDA(train,test):\n",
        "    \"\"\"\n",
        "    This function train the LDA model.\n",
        "    INPUT :  \n",
        "        data : csv file containing the points in R² and their label y.\n",
        "\n",
        "    OUTPUT : w : estimated weights \n",
        "             b : estimated bias\n",
        "\n",
        "    \"\"\"\n",
        "    x_train = train[:,:-1]\n",
        "    y_train = train[:,2]\n",
        "    \n",
        "    x_test = test[:,:-1]\n",
        "    y_test = test[:,2]\n",
        "\n",
        "    n1 = sum(y_train)\n",
        "    N = len(y_train)\n",
        "    pi = n1/N\n",
        "    mu_1 = np.sum(x_train[y_train == 1], axis=0)/n1\n",
        "    mu_0 = np.sum(x_train[y_train == 0], axis=0)/(N-n1)\n",
        "    sigma =1/N * ( np.dot((X_1-mu_1).T,(X_1-mu_1)) + \n",
        "                  np.dot((X_0-mu_0).T,(X_0-mu_0)))\n",
        "    \n",
        "    sigma_inv = np.linalg.inv(sigma)\n",
        "\n",
        "    beta = sigma_inv @(mu_1-mu_0)\n",
        "    alpha = -1/2 * (mu_1-mu_0).T @ sigma_inv @ (mu_1 + mu_0) +np.log(pi/(1-pi))\n",
        "\n",
        "    print(\"pi:\", round(pi,2), \"\\nmu_1:\", mu_1, \"\\nmu_0:\", mu_0, \"\\nSigma:\", \n",
        "          sigma, \"\\nbeta:\", beta, \"\\nalpha:\", round(alpha,2))\n",
        "\n",
        "    return beta, alpha \n",
        "\n",
        "\n",
        " def separation_line(train,test,alpha,beta):  \n",
        "\n",
        "    x_train = train[:,:2]\n",
        "    y_train = train[:,2]\n",
        "    \n",
        "    x_test = test[:,:-1]\n",
        "    y_test = test[:,2]\n",
        "    pi = sum(y_train)/len(y_train)\n",
        "    x_min, x_max = np.min(x_train[:,0]), np.max(x_train[:,0])\n",
        "     \n",
        "    boundary_x = np.linspace(x_min, x_max, 1000)\n",
        "    boundary_y = [(np.log((1-pi)/pi)- beta[0]*x-alpha)/beta[1] \n",
        "                  for x in boundary_x]\n",
        "    \n",
        "    Xt_1 = x_test[y_test == 1]\n",
        "    Xt_0 = x_test[y_test == 0]\n",
        "    \n",
        "    \n",
        "    xt_min, xt_max = np.min(x_test[:,0]), np.max(x_test[:,0])\n",
        "    boundary_xt = np.linspace(xt_min, xt_max, 1000)\n",
        "    boundary_yt = [(np.log((1-pi)/pi)- beta[0]*x-alpha)/beta[1] \n",
        "                  for x in boundary_x]\n",
        "\n",
        "    return boundary_x, boundary_y, boundary_xt, boundary_yt\n",
        "\n",
        "  def predict(train,test, alpha, beta):\n",
        "    pi = sum(y_train)/len(y_train)\n",
        "    pred = [(np.log(pi/(1-pi)) - alpha - beta[0]*x[0])/beta[1] \n",
        "          for x in test[:,:2]]\n",
        "    predict = test[:,1] > pred\n",
        "    misclass = 1-sum((predict != test[:,2]))/len(predict)\n",
        "\n",
        "    pred = [(np.log(pi/(1-pi)) - alpha - beta[0]*x[0])/beta[1] \n",
        "            for x in train[:,:2]]\n",
        "    predict = train[:,1] > pred\n",
        "    misclass_train = 1-sum((predict != train[:,2]))/len(predict)\n",
        "    \n",
        "    print(\"Misclassification on train set: \", misclass_train)\n",
        "    print(\"\\n\")\n",
        "    print(\"Misclassification on test set: \", misclass)\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBpz6M87PSpQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = \"LDA\"\n",
        "train, test = trainA, testA\n",
        "beta, alpha = train_LDA(train,test)\n",
        "boundary_x, boundary_y, boundary_xt, boundary_yt = separation_line(train,test,\n",
        "                                                                   alpha,beta)\n",
        "plot_classification(train,test,boundary_x,boundary_y,\"A\", linear = True,\n",
        "                        model,Z=0,Zt=0)\n",
        "predict(train,test, alpha, beta)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-m_qI8Smsjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, test = trainB, testB\n",
        "beta, alpha = train_LDA(train,test)\n",
        "boundary_x, boundary_y, boundary_xt, boundary_yt = separation_line(train,test,\n",
        "                                                                   alpha,beta)\n",
        "plot_classification(train,test,boundary_x,boundary_y,\"B\", linear = True,\n",
        "                        model,Z=0,Zt=0)\n",
        "predict(train,test, alpha, beta)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuXVhmTImsfJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, test = trainB, testB\n",
        "beta, alpha = train_LDA(train,test)\n",
        "boundary_x, boundary_y, boundary_xt, boundary_yt = separation_line(train,test,\n",
        "                                                                   alpha,beta)\n",
        "plot_classification(train,test,boundary_x,boundary_y,\"C\", linear = True,\n",
        "                        model,Z=0,Zt=0)\n",
        "\n",
        "predict(train,test, alpha, beta)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k63fxorWPS3r",
        "colab_type": "text"
      },
      "source": [
        "###  Logistic regression\n",
        "\n",
        "The objective of this subsection is to implement logistic regression for an affine function $f(x)=\\mathbf{w}^\\intercal \\mathbf{x}+\\mathbf{b}$.\n",
        "\n",
        "####  Implementation of the MLE for this model and representation of the data and the classification line defined by the equation $p(y=1|x)=0.5$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hjrcFhpPuhE",
        "colab_type": "text"
      },
      "source": [
        "Logistic regression works as follows.\n",
        "\n",
        "Given an iid sample $\\{(\\boldsymbol{x}^{(1)}, y^{(1)}), ..., (\\boldsymbol{x}^{(n)}, y^{(n)})\\}$ of random variables $x$  and $y$\n",
        "with \n",
        "* $\\boldsymbol{x}$ a $d-$dimensional vector $\\boldsymbol{x} = (x_1, ..., x_d)$. In the present case $d=2$.\n",
        "* $y|X=x$ follows a Bernoulli law with parameter $\\sigma (f(x)) = \\sigma (\\mathbf{w}^\\intercal \\mathbf{x}+\\mathbf{b}) $, $y \\in \\{0,1\\}$\n",
        "\n",
        "The log-likelihood of the sample is written :\n",
        "$ l(w,b) =$  $  \\sum_{i=1}^{n} \\{ y_i.log(\\sigma (f(x_i))  + (1-y_i).log(1 -\\sigma (f(x_i)) \\}$. The goal is to minimize this negative log-likelihood which means to find $\\hat{w}, \\hat{b}  = \\underset{w, b}{\\operatorname{argmax}} l(w,b)$.\n",
        "\n",
        "To find the weights and the bias, we will implement a first order gradient ascent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tlIfZslROmw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient_ascent(n_iter, data, lrate):\n",
        "    \"\"\"\n",
        "    This function is a first order gradient ascent algorithm which estimates \n",
        "    the weights and the bias of this model.\n",
        "    INPUT :  \n",
        "        data : ndarray conatining points in R² and their label y.\n",
        "        n_iters = number of iterations through which the update of parameters \n",
        "        using gradient descent will be done\n",
        "        learning_rate : the step size used for the gradient ascent algorithm\n",
        "    OUTPUT : w : estimated weights \n",
        "             b : estimated bias\n",
        "\n",
        "    \"\"\"\n",
        "    x = data[:,:2]\n",
        "    y= data[:,2].reshape(-1,1)\n",
        "    #N = len(data)\n",
        "    w = np.zeros(shape=(np.shape(x)[1],1))\n",
        "    b = 0\n",
        "    for i in range(n_iter):\n",
        "        y_pred = sigm(np.dot(x,w) + b)\n",
        "        grad_w = np.dot(x.T, y-y_pred)\n",
        "        grad_b = sum(y-y_pred)\n",
        "        w = w + lrate*grad_w\n",
        "        b = b + lrate*grad_b\n",
        "    return w, b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59_pOUnBRRqS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sep_logreg(train, test):            \n",
        "    w,b = gradient_ascent(100, train, 0.01)\n",
        "    \n",
        "    x_train = train[:,:-1]\n",
        "    y_train = train[:,2]\n",
        "    \n",
        "    x_test = test[:,:-1]\n",
        "    y_test = test[:,2]\n",
        "    \n",
        "    X_1 = x_train[y_train == 1]\n",
        "    X_0 = x_train[y_train == 0]\n",
        "    \n",
        "    x_min, x_max = np.min(x_train[:,0]), np.max(x_train[:,0])\n",
        "         \n",
        "    boundary_x = np.linspace(x_min, x_max, 1000)\n",
        "    boundary_y = [(b- w[0]*x)/w[1] for x in boundary_x]\n",
        "\n",
        "    Xt_1 = x_test[y_test == 1]\n",
        "    Xt_0 = x_test[y_test == 0]\n",
        "    \n",
        "    \n",
        "    xt_min, xt_max = np.min(x_test[:,0]), np.max(x_test[:,0])\n",
        "    boundary_xt = np.linspace(xt_min, xt_max, 1000)\n",
        "    boundary_yt = [(b- w[0]*x)/w[1] for x in boundary_x]\n",
        "\n",
        "    return boundary_x, boundary_y, boundary_xt, boundary_yt\n",
        "\n",
        "\n",
        "\n",
        "def pred_logreg(train, test):\n",
        "  pred = [(b- w[0]*x[0])/w[1] for x in te st[:,:2]]\n",
        "  predict = test[:,1].reshape(-1,1) > pred\n",
        "  misclass = 1-sum((predict != test[:,2].reshape(-1,1)))/len(predict)\n",
        "\n",
        "  pred = [(b- w[0]*x[0])/w[1] for x in train[:,:2]]\n",
        "  predict = train[:,1].reshape(-1,1) > pred\n",
        "  misclass_train = 1-sum((predict != train[:,2].reshape(-1,1)))/len(predict)\n",
        "  print(\"Misclassification on train set: \", misclass_train)\n",
        "  print(\"\\n\")\n",
        "  print(\"Misclassification on test set: \", misclass)\n",
        "  print(\"\\n\")\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COIFigP80uh5",
        "colab_type": "text"
      },
      "source": [
        "Let's estimate parameters for each training set using the train function, and then let's determine the line defined by P(Y=1|x) = 0.5 for each training and testing sets. Then let's plot the datasets with the separating line.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP1F8WBDUb4C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = \"Logistic regression\"\n",
        "train, test = trainA, testA\n",
        "boundary_x, boundary_y, boundary_xt, boundary_yt = sep_logreg(train,test)\n",
        "plot_classification(train,test,boundary_x,boundary_y,\"A\", linear = True,\n",
        "                        model,Z=0,Zt=0)\n",
        "pred_logreg(train,test, alpha, beta)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGKc1i8vr3cr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, test = trainA, testA\n",
        "boundary_x, boundary_y, boundary_xt, boundary_yt = sep_logreg(train,test)\n",
        "plot_classification(train,test,boundary_x,boundary_y,\"B\", linear = True,\n",
        "                        model,Z=0,Zt=0)\n",
        "pred_logreg(train,test, alpha, beta)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fL5k9Pfzr3Z6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, test = trainA, testA\n",
        "boundary_x, boundary_y, boundary_xt, boundary_yt = sep_logreg(train,test)\n",
        "plot_classification(train,test,boundary_x,boundary_y,\"C\", linear = True,\n",
        "                        model,Z=0,Zt=0)\n",
        "pred_logreg(train,test, alpha, beta)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZB4KicmUcQg",
        "colab_type": "text"
      },
      "source": [
        "###  Linear regression\n",
        "\n",
        "The objective of this subsection is to implement linear regression for an affine function $f(x)=\\mathbf{w}^\\intercal \\mathbf{x}+\\mathbf{b}$ by solving normal equations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ui62zoL6U8FK",
        "colab_type": "text"
      },
      "source": [
        "Linear regression works as follows.\n",
        "\n",
        "Given an iid sample $\\{(\\boldsymbol{x}^{(1)}, y^{(1)}), ..., (\\boldsymbol{x}^{(n)}, y^{(n)})\\}$ of random variables $x$  and $y$\n",
        "with \n",
        "* $\\boldsymbol{x}$ is a $d-$dimensional vector $\\boldsymbol{x} = (x_1, ..., x_d)$. In the present case $d=2$.\n",
        "* $y_i = x_i^\\intercal .w+ b  = \\left(\n",
        "\\begin{array}{c}\n",
        "x_i\\\\\n",
        "1\\\\\n",
        "\\end{array}\n",
        "\\right) ^\\intercal. \\ \\left(\n",
        "\\begin{array}{c}\n",
        "w\\\\\n",
        "b\\\\\n",
        "\\end{array}\n",
        "\\right) .$\n",
        "\n",
        "This can be written in matricial terms : $ y= X.\\beta $ where the $i^{th}$ row of matrix $X$ is $ \\left( \\begin{array}{r} x_i\\\\ 1\\\\ \\end{array} \\right) ^T$; and $\\beta  = \\left(\n",
        "\\begin{array}{c}\n",
        "w\\\\\n",
        "b\\\\\n",
        "\\end{array}\n",
        "\\right)$. \n",
        "\n",
        " The objective is to find $\\hat{\\beta}  = \\underset{\\beta}{\\operatorname{argmin}} \\frac{1}{2n} \\left\\Vert y- X.\\beta \\right\\Vert^2 $. To solve this equation, we will solve normal equation : $X ^\\intercal X \\beta = X ^\\intercal y \\implies \\beta = (X^\\intercal.X)^{-1}.X^\\intercal.y $ . "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itbLg2zAXLea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_linreg(train,test)            \n",
        "            \n",
        "    x_train = train[:,:-1]\n",
        "    y_train = train[:,2]\n",
        "    \n",
        "    x_test = test[:,:-1]\n",
        "    y_test = test[:,2]\n",
        "    \n",
        "    X = np.append(x_train,np.ones(len(x_train)).reshape(-1,1),axis=1)\n",
        "    theta = np.linalg.inv(X.T @ X)@X.T @ y_train.reshape(-1,1)\n",
        "    w, b = theta[:2,],theta[2,]\n",
        "    return w, b \n",
        "    \n",
        "def sep_linreg(train,test, w, b )\n",
        "\n",
        "  x_train = train[:,:-1]\n",
        "  y_train = train[:,2]\n",
        "  \n",
        "  x_test = test[:,:-1]\n",
        "  y_test = test[:,2]\n",
        "  X_1 = x_train[y_train == 1]\n",
        "  X_0 = x_train[y_train == 0]\n",
        "\n",
        "  x_min, x_max = np.min(x_train[:,0]), np.max(x_train[:,0])\n",
        "      \n",
        "  boundary_x = np.linspace(x_min, x_max, 1000)\n",
        "  boundary_y = [(1/2 -b- w[0]*x)/w[1] for x in boundary_x]\n",
        "\n",
        "  xt_min, xt_max = np.min(x_test[:,0]), np.max(x_test[:,0])\n",
        "      \n",
        "  boundary_x = np.linspace(xt_min, xt_max, 1000)\n",
        "  boundary_y = [(1/2 -b- w[0]*x)/w[1] for x in boundary_x]\n",
        "\n",
        "\n",
        "def pred_linreg(train,test, w, b):\n",
        "\n",
        "  #compute misclassification for train_set\n",
        "  pred = [(1/2 -b- w[0]*x)/w[1] for x in train[:,0]]\n",
        "  predict = train[:,1].reshape(-1,1) > pred\n",
        "  misclass_train = 1-sum((predict != train[:,2].reshape(-1,1)))/len(predict)\n",
        "\n",
        "  #compute misclassification for test_set\n",
        "  pred = [(1/2 -b- w[0]*x)/w[1] for x in test[:,0]]\n",
        "  predict = test[:,1].reshape(-1,1) > pred\n",
        "  misclass = 1-sum((predict != test[:,2].reshape(-1,1)))/len(predict)\n",
        "  \n",
        "  print(\"w = \", w)\n",
        "  print(\"b = \", b)\n",
        "  \n",
        "  print(\"Misclassification on train set: \", misclass_train)\n",
        "  print(\"\\n\")\n",
        "  print(\"Misclassification on test set: \", misclass)\n",
        "  print(\"\\n\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f8oysca1Wkg",
        "colab_type": "text"
      },
      "source": [
        "Let's estimate parameters for each training set using the train function, and then let's determine the line defined by P(Y=1|x) = 0.5 for each training and testing sets. Then let's plot the datasets with the separating line. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmbZcTtKXXfr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = \"Linear regression\"\n",
        "train, test = trainA, testA\n",
        "w, b = train_linreg(train,test)\n",
        "boundary_x, boundary_y, boundary_xt, boundary_yt = sep_linreg(train,test,\n",
        "                                                                   w,b)\n",
        "plot_classification(train,test,boundary_x,boundary_y,\"A\", linear = True,\n",
        "                        model,Z=0,Zt=0)\n",
        "pred_linreg(train,test, w, b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9UAkyC1wXSy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = \"Linear regression\"\n",
        "train, test = trainA, testA\n",
        "w, b = train_linreg(train,test)\n",
        "boundary_x, boundary_y, boundary_xt, boundary_yt = sep_linreg(train,test,\n",
        "                                                                   w,b)\n",
        "plot_classification(train,test,boundary_x,boundary_y,\"B\", linear = True,\n",
        "                        model,Z=0,Zt=0)\n",
        "pred_linreg(train,test, w, b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1pryDVKwYPr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = \"Linear regression\"\n",
        "train, test = trainA, testA\n",
        "w, b = train_linreg(train,test)\n",
        "boundary_x, boundary_y, boundary_xt, boundary_yt = sep_linreg(train,test,\n",
        "                                                                   w,b)\n",
        "plot_classification(train,test,boundary_x,boundary_y,\"C\", linear = True,\n",
        "                        model,Z=0,Zt=0)\n",
        "pred_linreg(train,test, w, b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75Y4liqzXYSm",
        "colab_type": "text"
      },
      "source": [
        "### QDA model\n",
        "\n",
        "We finally relax the assumption that the covariance matrices for the two classes are the same. So, given the class label the data are assumed to be Gaussian with means and covariance matrices which are a priori different.\n",
        "$ y \\sim \\mathcal{B} {(\\pi)}$ , $x|y=i$ $\\sim \\mathcal{N}(\\mu_i ,\\,\\Sigma _i)\\,$\n",
        "\n",
        "The goal of this subsection is to implement the maximum likelihood estimator of this model and apply it to the data. \n",
        "\n",
        "\n",
        "####  Implementation of the MLE for this model and representation of the data and the classification line defined by the equation $p(y=1|x)=0.5$ :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZramPlD9X40J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_QDA(train,test):\n",
        "    x_train = train[:,:-1]\n",
        "    y_train = train[:,2]\n",
        "    \n",
        "    y = y_train.reshape(-1,1)\n",
        "    x_test = test[:,:-1]\n",
        "    y_test = test[:,2]\n",
        "    \n",
        "    X_1 = x_train[y_train == 1]\n",
        "    X_0 = x_train[y_train == 0]\n",
        "\n",
        "    n1 = sum(y_train)\n",
        "    N = len(y_train)\n",
        "    pi = n1/N\n",
        "    mu_1 = np.sum(X_1, axis=0)/n1\n",
        "    mu_0 = np.sum(X_0, axis=0)/(N-n1)\n",
        "    sigma_1 =1/n1 * np.dot((X_1-mu_1).T,(X_1-mu_1))\n",
        "    sigma_0 = 1/(N-n1) * np.dot((X_0-mu_0).T,(X_0-mu_0))\n",
        "    \n",
        "    sigma0_inv = np.linalg.inv(sigma_0)\n",
        "    sigma1_inv = np.linalg.inv(sigma_1)\n",
        "    \n",
        "    Q = sigma0_inv - sigma1_inv\n",
        "    Q=Q/2\n",
        "    \n",
        "    \n",
        "    beta = sigma1_inv @ mu_1 - sigma0_inv @ mu_0 \n",
        "    alpha = 1/2 * (mu_0.T @ sigma0_inv @  mu_0 - mu_1.T @ sigma1_inv @  mu_1) \n",
        "     + np.log(pi/(1-pi)) + 1/2 * (np.log(np.linalg.det(sigma_0)) -\n",
        "                                  np.log(np.linalg.det(sigma_1)))\n",
        "    \n",
        "    print(\"Q\", round(Q,2), \"\\nbeta:\", round(beta,2), \"\\nalpha:\", round(alpha,2))\n",
        "    return Q, beta, alpha\n",
        "\n",
        "\n",
        "def f(x,Q,beta,alpha):\n",
        "        return Q[0,0]*x[0]**2 + x[0]*x[1]*(Q[1,0] + Q[0,1]) + Q[1,1]*x[1]**2 +\n",
        "         beta[0]* x[0] + beta[1]* x[1] + alpha\n",
        "\n",
        "def sep_line(train,test, Q, beta, alpha):\n",
        "\n",
        "    x_train = train[:,:-1]\n",
        "    y_train = train[:,2]\n",
        "    \n",
        "    y = y_train.reshape(-1,1)\n",
        "    x_test = test[:,:-1]\n",
        "    y_test = test[:,2]\n",
        "\n",
        "    x_min, x_max = np.min(x_train[:,0]), np.max(x_train[:,0])\n",
        "    y_min, y_max = np.min(x_train[:,1]), np.max(x_train[:,1])\n",
        "    \n",
        "     \n",
        "    boundary_x0 = np.linspace(x_min, x_max, 1000)\n",
        "    boundary_x1 = np.linspace(y_min, y_max, 1000)\n",
        "    XX = np.meshgrid(boundary_x0, boundary_x1)\n",
        "    Z = f(XX,Q,beta,alpha)\n",
        "\n",
        "    \n",
        "    Xt_1 = x_test[y_test == 1]\n",
        "    Xt_0 = x_test[y_test == 0]\n",
        "    \n",
        "    x_min, x_max = np.min(x_test[:,0]), np.max(x_test[:,0])\n",
        "    y_min, y_max = np.min(x_test[:,1]), np.max(x_test[:,1])\n",
        "         \n",
        "    boundary_xt0 = np.linspace(x_min, x_max, 1000)\n",
        "    boundary_xt1 = np.linspace(y_min, y_max, 1000)\n",
        "    XXt = np.meshgrid(boundary_xt0,boundary_xt1)\n",
        "    Zt = f(XXt,Q, beta, alpha)\n",
        "\n",
        "    return boundary_x0, boundary_x1, boundary_xt0, boundary_xt1, Z, Zt\n",
        "\n",
        "\n",
        "def pred_QDA(train, test, Q, beta, alpha):\n",
        "    x_train = train[:,:-1]\n",
        "    y_train = train[:,2]\n",
        "    \n",
        "    y = y_train.reshape(-1,1)\n",
        "    x_test = test[:,:-1]\n",
        "    y_test = test[:,2]\n",
        "    pred = np.array([f(i,Q,beta,alpha) for i in x_train])\n",
        "    pred = np.where(np.array(pred)>0,0,1)\n",
        "    misclassification = 1- sum(pred != y_train)/len(y_train)\n",
        "    \n",
        "    pred = np.array([f(i,Q, beta, alpha) for i in x_test])\n",
        "    pred = np.where(np.array(pred)>0,0,1)\n",
        "    misclass = 1- sum(pred != y_test)/len(y_test)\n",
        "\n",
        "    print(\"Misclassification on train set: \", misclassification)\n",
        "    print(\"\\n\")\n",
        "    print(\"Misclassification on test set: \", misclass)\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDGN1F4e1ZAz",
        "colab_type": "text"
      },
      "source": [
        "Let's estimate parameters for each training set using the train function, and then let's determine the line defined by P(Y=1|x) = 0.5 for each training and testing sets. Then let's plot the datasets with the separating line. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1GZeMxIy610",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = \"QDA\"\n",
        "train, test = trainA, testA\n",
        "Q, beta, alpha = train_QDA(train,test)\n",
        "boundary_x, boundary_y, boundary_xt, boundary_yt, Z, Zt = sep_line(train,test, Q, beta, alpha)\n",
        "plot_classification(train,test,boundary_x,boundary_y,\"A\", linear = False,\n",
        "                        model,Z=Z,Zt=Zt)\n",
        "pred_linreg(train,test, Q, beta, alpha)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJgOeK3ty7tf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = \"QDA\"\n",
        "train, test = trainA, testA\n",
        "Q, beta, alpha = train_QDA(train,test)\n",
        "boundary_x, boundary_y, boundary_xt, boundary_yt, Z, Zt = sep_line(train,test, Q, beta, alpha)\n",
        "plot_classification(train,test,boundary_x,boundary_y,\"B\", linear = False,\n",
        "                        model,Z=Z,Zt=Zt)\n",
        "pred_linreg(train,test, Q, beta, alpha)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ks7REaYhy8B8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = \"QDA\"\n",
        "train, test = trainA, testA\n",
        "Q, beta, alpha = train_QDA(train,test)\n",
        "boundary_x, boundary_y, boundary_xt, boundary_yt, Z, Zt = sep_line(train,test, Q, beta, alpha)\n",
        "plot_classification(train,test,boundary_x,boundary_y,\"C\", linear = False,\n",
        "                        model,Z=Z,Zt=Zt)\n",
        "pred_linreg(train,test, Q, beta, alpha)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}